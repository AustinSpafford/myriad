// We must identify the kernel (entry-point) functions.
#pragma kernel kernel_build_unsorted_voxel_particle_pairs

#include "spatializer_shader_types.cginc"

uniform uint u_particle_count;

uniform uint u_voxel_count_per_axis;
uniform float u_voxel_size;

uniform StructuredBuffer<s_particle_position> u_particle_positions;

uniform StructuredBuffer<s_voxel_particle_pair> u_readable_sorted_voxel_particle_pairs;
uniform RWStructuredBuffer<s_voxel_particle_pair> u_out_next_sorted_voxel_particle_pairs;

uniform RWStructuredBuffer<s_spatialization_voxel> u_out_spatialization_voxels;
uniform RWStructuredBuffer<s_neighborhood> u_out_neighborhoods;

uint3 wrap_voxel_coord(
	int3 unbounded_voxel_coord)
{
	// Which wrapping of the voxel-grid are we in?
	int3 macro_voxel_coordinate = (unbounded_voxel_coord / u_voxel_count_per_axis);

	// Determine our position within the macro-voxel. 
	// Note that this is different than using truncation, because we want -1 and 1 to map to
	// different values (to avoid excessive collisions around the origin).
	uint3 wrapped_voxel_coord = (
		unbounded_voxel_coord - 
		(macro_voxel_coordinate * u_voxel_count_per_axis));

	return wrapped_voxel_coord;
}

uint get_voxel_index_from_voxel_coord(
	uint3 wrapped_voxel_coord)
{
	return (
		wrapped_voxel_coord.x +
		(wrapped_voxel_coord.y * u_voxel_count_per_axis) +
		(wrapped_voxel_coord.z * (u_voxel_count_per_axis * u_voxel_count_per_axis)));
}

[numthreads(128, 1, 1)]
void kernel_build_unsorted_voxel_particle_pairs(
	uint3 thread_id : SV_DispatchThreadID)
{
	uint particle_index = thread_id.x;
	
	// In the last thread group, if it's safe to access the buffers.
	if (particle_index < u_particle_count)
	{
		float3 position_in_voxel_space = (u_particle_positions[particle_index].position / u_voxel_size);

		int3 unbounded_voxel_coord = (int3)floor(position_in_voxel_space);

		// Which wrapping of the voxel-grid are we in?
		int3 macro_voxel_coordinate = (unbounded_voxel_coord / u_voxel_count_per_axis);

		// Determine our position within the meta-voxel. Note that this is different than using truncation,
		// because we want -1 and 1 to map to different values (to avoid excessive collisions around the origin).
		uint3 wrapped_voxel_coord = (
			unbounded_voxel_coord - 
			(macro_voxel_coordinate * u_voxel_count_per_axis));

		float3 position_fraction = (position_in_voxel_space - unbounded_voxel_coord);

		// Create an offset that's (-1, -1, -1) in the lowest-corner of each voxel, and (0, 0, 0) in
		// the highest-corner. This allows us to specify each particle's
		// neighborhood (which is a group of 8 adjacent voxels) by the coordinate of the lowest-corner voxel.
		int3 offset_from_particle_voxel_to_neighborhood_min_voxel_coord = 
			(int3)floor(position_fraction - 0.5f);

		// Output results.
		{
			// NOTE: We're providing the initial particle-sorted pairings, which will
			// soon become voxel-sorted so they're spatially-relevent.

			u_out_next_sorted_voxel_particle_pairs[particle_index].voxel_index = 
				get_voxel_index_from_voxel_coord(wrapped_voxel_coord);

			u_out_next_sorted_voxel_particle_pairs[particle_index].particle_index = 
				particle_index;

			u_out_neighborhoods[particle_index].neighborhood_min_voxel_coord = 
				wrap_voxel_coord(
					wrapped_voxel_coord +
					offset_from_particle_voxel_to_neighborhood_min_voxel_coord);
		}
	}
}

/* SCRATCH
Our buffers:
[particle_count] { particle_position } 
[particle_count] { sort_key_voxel_index, particle_index }
[voxel_count] { sorted_particle_index }
[particle_count] { access_min_voxel_coord } 

Kernels:
Import: positions -> { particle_index, voxel_index }
Sort by voxel_index.
Find each voxel's position in the particle indices.
Export: particle indices, voxels, min_voxel_coords.
*/
